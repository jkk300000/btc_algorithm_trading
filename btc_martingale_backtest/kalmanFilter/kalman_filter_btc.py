import numpy as np
import pandas as pd
from scipy.linalg import inv
import logging
from typing import Dict, Tuple, Optional
from joblib import Parallel, delayed
import multiprocessing



# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.hasHandlers():
    handler = logging.StreamHandler()
    formatter = logging.Formatter('[%(asctime)s][%(levelname)s] %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# GPU ê°€ì†ì„ ìœ„í•œ CuPy ì§€ì› (ì„ íƒì )
try:
    import cupy as cp
    CUPY_AVAILABLE = True
    logger.info("âœ… CuPy GPU ê°€ì† ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    cp = np  # CuPyê°€ ì—†ìœ¼ë©´ NumPy ì‚¬ìš©
    CUPY_AVAILABLE = False
    logger.info("âš ï¸ CuPy ì—†ìŒ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")




class KalmanFilter:
    """ê¸°ë³¸ ì¹¼ë§Œ í•„í„° í´ë˜ìŠ¤"""
    
    def __init__(self, initial_state: float, initial_P: float = 1.0, 
                 Q: float = 0.01, R: float = 1.0):
        """
        ì¹¼ë§Œ í•„í„° ì´ˆê¸°í™”
        
        Args:
            initial_state: ì´ˆê¸° ìƒíƒœ (ê°€ê²©)
            initial_P: ì´ˆê¸° ìƒíƒœ ë¶ˆí™•ì‹¤ì„±
            Q: í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ (ì‹œìŠ¤í…œ ë…¸ì´ì¦ˆ)
            R: ì¸¡ì • ë…¸ì´ì¦ˆ (ê´€ì¸¡ ë…¸ì´ì¦ˆ)
        """
        self.x = initial_state  # ìƒíƒœ ì¶”ì •ê°’
        self.P = initial_P      # ìƒíƒœ ë¶ˆí™•ì‹¤ì„±
        self.Q = Q              # í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ
        self.R = R              # ì¸¡ì • ë…¸ì´ì¦ˆ
        
        # ìƒíƒœ ì „ì´ í–‰ë ¬ (1ì°¨ì› ê°€ê²© ëª¨ë¸)
        self.F = 1.0            # ìƒíƒœ ì „ì´
        self.H = 1.0            # ê´€ì¸¡ í–‰ë ¬
        
    def predict(self):
        """ì˜ˆì¸¡ ë‹¨ê³„"""
        # ìƒíƒœ ì˜ˆì¸¡
        self.x = self.F * self.x
        # ë¶ˆí™•ì‹¤ì„± ì˜ˆì¸¡
        self.P = self.F * self.P * self.F + self.Q
        
    def update(self, measurement: float) -> float:
        """ì—…ë°ì´íŠ¸ ë‹¨ê³„"""
        # ì¹¼ë§Œ ê²Œì¸ ê³„ì‚°
        K = self.P * self.H / (self.H * self.P * self.H + self.R)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.x = self.x + K * (measurement - self.H * self.x)
        
        # ë¶ˆí™•ì‹¤ì„± ì—…ë°ì´íŠ¸
        self.P = (1 - K * self.H) * self.P
        
        return self.x


class AdaptiveBTCKalmanFilter:
    """ë¹„íŠ¸ì½”ì¸ ë¬´ê¸°í•œ ì„ ë¬¼ìš© ì ì‘í˜• ì¹¼ë§Œ í•„í„°"""
    
    def __init__(self, base_Q: float = 0.005, base_R: float = 2.0, 
                 volatility_window: int = 20, volatility_threshold: float = 0.08,
                 preservation_factor: float = 0.75, adaptive_factor: float = 5.0,
                 use_gpu: bool = False, n_jobs: int = 1):
        """
        ë¹„íŠ¸ì½”ì¸ìš© ì ì‘í˜• ì¹¼ë§Œ í•„í„° ì´ˆê¸°í™”
        
        Args:
            base_Q: ê¸°ë³¸ í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ
            base_R: ê¸°ë³¸ ì¸¡ì • ë…¸ì´ì¦ˆ
            volatility_window: ë³€ë™ì„± ê³„ì‚° ìœˆë„ìš°
            volatility_threshold: ê·¹í•œ ë³€ë™ì„± ì„ê³„ê°’ (8%)
            preservation_factor: ì›ë³¸ ì‹ í˜¸ ë³´ì¡´ ë¹„ìœ¨ (75%)
            adaptive_factor: ì ì‘í˜• ì¡°ì • ê³„ìˆ˜
            use_gpu: GPU ê°€ì† ì‚¬ìš© ì—¬ë¶€ (CuPy í•„ìš”)
            n_jobs: ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (-1: ëª¨ë“  CPU ì‚¬ìš©)
        """
        self.base_Q = base_Q
        self.base_R = base_R
        self.volatility_window = volatility_window
        self.volatility_threshold = volatility_threshold
        self.preservation_factor = preservation_factor
        self.adaptive_factor = adaptive_factor
        self.filters = {}
        self.volatility_history = []
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.use_gpu = use_gpu and CUPY_AVAILABLE
        self.n_jobs = n_jobs if n_jobs != -1 else multiprocessing.cpu_count()
        
        # GPU ì‚¬ìš© ì—¬ë¶€ ë¡œê¹…
        if self.use_gpu:
            logger.info(f"ğŸš€ GPU ê°€ì† ëª¨ë“œ í™œì„±í™” (CuPy)")
        if self.n_jobs > 1:
            logger.info(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ í™œì„±í™” ({self.n_jobs} í”„ë¡œì„¸ìŠ¤)")
    
    def _get_array_lib(self):
        """GPU/CPU ë°°ì—´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ"""
        return cp if self.use_gpu else np
    
    def _to_gpu(self, data):
        """GPUë¡œ ë°ì´í„° ì´ë™ (GPU ëª¨ë“œì¼ ë•Œë§Œ)"""
        if self.use_gpu and isinstance(data, np.ndarray):
            return cp.asarray(data)
        return data
    
    def _to_cpu(self, data):
        """CPUë¡œ ë°ì´í„° ì´ë™ (ê²°ê³¼ ë°˜í™˜ìš©)"""
        if self.use_gpu and hasattr(data, 'get'):
            return data.get()
        return data
        
    def calculate_adaptive_parameters(self, price_series: pd.Series, 
                                    current_idx: int) -> Tuple[float, float]:
        """ë³€ë™ì„± ê¸°ë°˜ ì ì‘í˜• íŒŒë¼ë¯¸í„° ê³„ì‚°"""
        
        if current_idx < self.volatility_window:
            # ì´ˆê¸° êµ¬ê°„: ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
            return self.base_Q, self.base_R
        
        # ìµœê·¼ ë³€ë™ì„± ê³„ì‚°
        recent_prices = price_series.iloc[current_idx-self.volatility_window:current_idx+1]
        returns = np.diff(np.log(recent_prices))
        current_volatility = np.std(returns, ddof=1)
        
        # ë³€ë™ì„± ì •ê·œí™” (0~1 ë²”ìœ„)
        normalized_vol = min(current_volatility / 0.1, 1.0)  # 10% ë³€ë™ì„±ì„ ìµœëŒ€ê°’ìœ¼ë¡œ
        
        # ì ì‘í˜• íŒŒë¼ë¯¸í„° ê³„ì‚°
        adaptive_Q = self.base_Q * (1 + normalized_vol * self.adaptive_factor)
        adaptive_R = self.base_R * (1 + normalized_vol * (self.adaptive_factor * 0.5))
        
        return adaptive_Q, adaptive_R
    
    def detect_extreme_volatility(self, price_series: pd.Series, 
                                 window: int = 5) -> bool:
        """ê·¹í•œ ë³€ë™ì„± êµ¬ê°„ ê°ì§€"""
        if len(price_series) < window:
            return False
        
        recent_prices = price_series.iloc[-window:]
        returns = np.diff(np.log(recent_prices))
        volatility = np.std(returns, ddof=1)
        
        return volatility > self.volatility_threshold
    
    def _filter_single_column(self, df: pd.DataFrame, col: str) -> Tuple[list, list]:
        """ë‹¨ì¼ ì»¬ëŸ¼ í•„í„°ë§ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        xp = self._get_array_lib()  # GPU/CPU ë°°ì—´ ë¼ì´ë¸ŒëŸ¬ë¦¬
        
        # í•„í„° ì´ˆê¸°í™”
        if col not in self.filters:
            self.filters[col] = KalmanFilter(
                initial_state=df[col].iloc[0],
                Q=self.base_Q,
                R=self.base_R
            )
        
        filtered_values = []
        volatility_tracker = []
        
        # GPU ê°€ì† ì‹œ ë°ì´í„°ë¥¼ GPU ë©”ëª¨ë¦¬ë¡œ ì´ë™
        col_values = self._to_gpu(df[col].values) if self.use_gpu else df[col].values
        total_rows = len(col_values)
        
        for i, value in enumerate(col_values):
            # ì§„í–‰ë¥  í‘œì‹œ (ë§¤ 10000í–‰ë§ˆë‹¤)
            if i > 0 and i % 10000 == 0:
                progress = (i / total_rows) * 100
                logger.info(f"  ğŸ“ˆ {col} ì»¬ëŸ¼ ì§„í–‰ë¥ : {progress:.1f}% ({i:,}/{total_rows:,})")
            
            # ê·¹í•œ ë³€ë™ì„± êµ¬ê°„ ê°ì§€ (CPUì—ì„œ ìˆ˜í–‰)
            cpu_value = self._to_cpu(value) if self.use_gpu else value
            price_history = df[col].iloc[:i+1]
            is_extreme_vol = self.detect_extreme_volatility(price_history)
            
            # ì ì‘í˜• íŒŒë¼ë¯¸í„° ê¸°ë³¸ê°’ ì„¤ì •
            adaptive_Q = self.base_Q
            adaptive_R = self.base_R
            
            if is_extreme_vol:
                # ê·¹í•œ ë³€ë™ì„±: ì›ë³¸ ì‹ í˜¸ ë³´ì¡´
                if i > 0:
                    filtered_value = (
                        self.preservation_factor * cpu_value + 
                        (1 - self.preservation_factor) * self.filters[col].x
                    )
                else:
                    filtered_value = cpu_value
            else:
                # ì¼ë°˜ ë³€ë™ì„±: ì ì‘í˜• ì¹¼ë§Œ í•„í„° ì ìš©
                adaptive_Q, adaptive_R = self.calculate_adaptive_parameters(df[col], i)
                
                # í•„í„° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
                self.filters[col].Q = adaptive_Q
                self.filters[col].R = adaptive_R
                
                # ì¹¼ë§Œ í•„í„° ì ìš©
                self.filters[col].predict()
                filtered_value = self.filters[col].update(cpu_value)
            
            filtered_values.append(filtered_value)
            volatility_tracker.append(adaptive_Q)
        
        return filtered_values, volatility_tracker
    
    def filter_ohlcv(self, df: pd.DataFrame) -> pd.DataFrame:
        """OHLCV ë°ì´í„° ì ì‘í˜• í•„í„°ë§ (GPU ê°€ì† ë° ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)"""
        filtered_df = df.copy()
        
        logger.info("ğŸš€ ì ì‘í˜• ì¹¼ë§Œ í•„í„° (1ë‹¨ê³„) ì‹œì‘")
        logger.info(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {len(df):,} í–‰")
        
        columns = ['open', 'high', 'low', 'close']
        
        if self.n_jobs > 1 and len(df) > 10000:  # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œë§Œ ë³‘ë ¬ ì²˜ë¦¬
            logger.info(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œë¡œ {len(columns)}ê°œ ì»¬ëŸ¼ ë™ì‹œ ì²˜ë¦¬")
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ì»¬ëŸ¼ ë™ì‹œ ì²˜ë¦¬
            results = Parallel(n_jobs=self.n_jobs, prefer="threads")(
                delayed(self._filter_single_column)(df, col) for col in columns
            )
            
            # ê²°ê³¼ ë³‘í•©
            for i, col in enumerate(columns):
                filtered_values, volatility_tracker = results[i]
                filtered_df[col] = filtered_values
                filtered_df[f'{col}_volatility'] = volatility_tracker
                logger.info(f"âœ… {col} ì»¬ëŸ¼ ì²˜ë¦¬ ì™„ë£Œ")
                
        else:
            # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            for col_idx, col in enumerate(columns):
                logger.info(f"ğŸ”„ {col} ì»¬ëŸ¼ ì²˜ë¦¬ ì¤‘... ({col_idx + 1}/4)")
                filtered_values, volatility_tracker = self._filter_single_column(df, col)
                filtered_df[col] = filtered_values
                filtered_df[f'{col}_volatility'] = volatility_tracker
                logger.info(f"âœ… {col} ì»¬ëŸ¼ ì²˜ë¦¬ ì™„ë£Œ")
        
        # ê±°ë˜ëŸ‰ í•„í„°ë§ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
        if 'volume' in df.columns:
            filtered_df['volume'] = self.filter_volume(df['volume'])
        
        logger.info("âœ… ì ì‘í˜• ì¹¼ë§Œ í•„í„° (1ë‹¨ê³„) ì™„ë£Œ")
        return filtered_df
    
    def filter_volume(self, volume_series: pd.Series) -> pd.Series:
        """ê±°ë˜ëŸ‰ í•„í„°ë§ (ë¡œê·¸ ìŠ¤ì¼€ì¼)"""
        log_volume = np.log(volume_series + 1)  # +1 to avoid log(0)
        
        if 'volume' not in self.filters:
            self.filters['volume'] = KalmanFilter(
                initial_state=log_volume.iloc[0],
                Q=self.base_Q * 0.1,  # ê±°ë˜ëŸ‰ì€ ë” ì‘ì€ ë…¸ì´ì¦ˆ
                R=self.base_R * 2.0   # ê±°ë˜ëŸ‰ì€ ë” í° ì¸¡ì • ë…¸ì´ì¦ˆ
            )
        
        filtered_log_volume = []
        for value in log_volume.values:
            self.filters['volume'].predict()
            filtered_value = self.filters['volume'].update(value)
            filtered_log_volume.append(filtered_value)
        
        # ë¡œê·¸ ì—­ë³€í™˜
        return np.exp(filtered_log_volume) - 1


class MultiScaleKalmanFilter:
    """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì¹¼ë§Œ í•„í„°"""
    
    def __init__(self, scales: list = [1, 5, 15]):
        """
        ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì¹¼ë§Œ í•„í„° ì´ˆê¸°í™”
        
        Args:
            scales: ë¶„ì„ ìŠ¤ì¼€ì¼ (1ë¶„, 5ë¶„, 15ë¶„)
        """
        self.scales = scales
        self.filters = {}
        
    def multi_scale_filtering(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ í•„í„°ë§"""
        filtered_df = df.copy()
        
        logger.info("ğŸ”¢ 2ë‹¨ê³„: ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì¹¼ë§Œ í•„í„° ì ìš© ì‹œì‘")
        logger.info(f"ğŸ“ ì ìš© ìŠ¤ì¼€ì¼: {self.scales}")
        
        for col in ['open', 'high', 'low', 'close']:
            # ê° ìŠ¤ì¼€ì¼ë³„ í•„í„° ì´ˆê¸°í™”
            scale_filters = {}
            for scale in self.scales:
                scale_filters[scale] = KalmanFilter(
                    initial_state=df[col].iloc[0],
                    Q=0.01 * scale,  # ìŠ¤ì¼€ì¼ì— ë¹„ë¡€í•˜ëŠ” ë…¸ì´ì¦ˆ
                    R=1.0 / scale    # ìŠ¤ì¼€ì¼ì— ë°˜ë¹„ë¡€í•˜ëŠ” ì¸¡ì • ë…¸ì´ì¦ˆ
                )
            
            filtered_values = []
            
            for i, value in enumerate(df[col].values):
                # ê° ìŠ¤ì¼€ì¼ë³„ í•„í„°ë§
                scale_predictions = {}
                for scale in self.scales:
                    if i >= scale:
                        # ìŠ¤ì¼€ì¼ë³„ ë°ì´í„° ì¶”ì¶œ
                        scale_data = df[col].iloc[i-scale:i+1]
                        scale_filter = scale_filters[scale]
                        
                        # ìŠ¤ì¼€ì¼ë³„ ì˜ˆì¸¡
                        scale_filter.predict()
                        scale_predictions[scale] = scale_filter.update(value)
                    else:
                        scale_predictions[scale] = value
                
                # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡
                weights = [1/scale for scale in self.scales]
                total_weight = sum(weights)
                final_prediction = sum(
                    pred * weight for pred, weight in zip(scale_predictions.values(), weights)
                ) / total_weight
                
                filtered_values.append(final_prediction)
            
            filtered_df[col] = filtered_values
        
        logger.info("âœ… 2ë‹¨ê³„: ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì¹¼ë§Œ í•„í„° ì ìš© ì™„ë£Œ")
        return filtered_df


def create_btc_kalman_filter(params: Optional[Dict] = None, use_gpu: bool = False, n_jobs: int = 1) -> AdaptiveBTCKalmanFilter:
    """ë¹„íŠ¸ì½”ì¸ ì „ìš© ì¹¼ë§Œ í•„í„° ìƒì„±"""
    
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
    default_params = {
        'base_Q': 0.005,           # ì‘ì€ í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ
        'base_R': 2.0,             # í° ì¸¡ì • ë…¸ì´ì¦ˆ
        'volatility_threshold': 0.08,  # 8% ë³€ë™ì„± ì„ê³„ê°’
        'preservation_factor': 0.75,   # 75% ì›ë³¸ ì‹ í˜¸ ë³´ì¡´
        'volatility_window': 20,       # 20ë¶„ ë³€ë™ì„± ìœˆë„ìš°
        'adaptive_factor': 5.0,        # ì ì‘í˜• ì¡°ì • ê³„ìˆ˜
        'use_gpu': use_gpu,           # GPU ê°€ì† ì‚¬ìš©
        'n_jobs': n_jobs              # ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
    }
    
    if params:
        default_params.update(params)
    
    return AdaptiveBTCKalmanFilter(**default_params)


def apply_btc_kalman_filtering(df: pd.DataFrame, 
                              use_multi_scale: bool = False,
                              params: Optional[Dict] = None,
                              use_gpu: bool = False,
                              n_jobs: int = 1) -> pd.DataFrame:
    """ë¹„íŠ¸ì½”ì¸ ë°ì´í„° ì¹¼ë§Œ í•„í„° ì ìš©"""
    
    logger.info("ğŸ¯ ë¹„íŠ¸ì½”ì¸ ì¹¼ë§Œ í•„í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    
    # 1ë‹¨ê³„: ì ì‘í˜• ì¹¼ë§Œ í•„í„° ì ìš©
    logger.info("ğŸ”„ 1ë‹¨ê³„: ì ì‘í˜• ì¹¼ë§Œ í•„í„° ì ìš©")
    adaptive_filter = create_btc_kalman_filter(params, use_gpu=use_gpu, n_jobs=n_jobs)
    df_filtered = adaptive_filter.filter_ohlcv(df)
    logger.info("âœ… 1ë‹¨ê³„: ì ì‘í˜• ì¹¼ë§Œ í•„í„° ì™„ë£Œ")
    
    # 2ë‹¨ê³„: ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ í•„í„°ë§ (ì„ íƒì )
    if use_multi_scale:
        multi_scale_filter = MultiScaleKalmanFilter()
        df_filtered = multi_scale_filter.multi_scale_filtering(df_filtered)
    
    logger.info("ğŸ ë¹„íŠ¸ì½”ì¸ ì¹¼ë§Œ í•„í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
    return df_filtered


def validate_kalman_performance(df_original: pd.DataFrame, 
                               df_filtered: pd.DataFrame) -> Dict:
    """ì¹¼ë§Œ í•„í„° ì„±ëŠ¥ ê²€ì¦"""
    
    logger.info("ğŸ“Š ì¹¼ë§Œ í•„í„° ì„±ëŠ¥ ê²€ì¦ ì‹œì‘")
    
    # ë³€ë™ì„± ë³´ì¡´ í™•ì¸
    original_volatility = df_original['close'].pct_change().rolling(20).std()
    filtered_volatility = df_filtered['close'].pct_change().rolling(20).std()
    
    # ê·¹í•œ ë³€ë™ì„± êµ¬ê°„ í™•ì¸
    extreme_vol_periods = original_volatility > 0.1  # 10% ì´ìƒ ë³€ë™ì„±
    
    # ê·¹í•œ êµ¬ê°„ì—ì„œì˜ ì‹ í˜¸ ë³´ì¡´ìœ¨
    if extreme_vol_periods.sum() > 0:
        preservation_ratio = (
            filtered_volatility[extreme_vol_periods] / 
            original_volatility[extreme_vol_periods]
        ).mean()
    else:
        preservation_ratio = 1.0
    
    # ë…¸ì´ì¦ˆ ì œê±° íš¨ê³¼
    noise_reduction = (
        original_volatility.mean() - filtered_volatility.mean()
    ) / original_volatility.mean()
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info(f"ê·¹í•œ ë³€ë™ì„± êµ¬ê°„ ì‹ í˜¸ ë³´ì¡´ìœ¨: {preservation_ratio:.2%}")
    logger.info(f"ë…¸ì´ì¦ˆ ì œê±° íš¨ê³¼: {noise_reduction:.2%}")
    
    return {
        'preservation_ratio': preservation_ratio,
        'noise_reduction': noise_reduction,
        'original_volatility_mean': original_volatility.mean(),
        'filtered_volatility_mean': filtered_volatility.mean(),
        'extreme_vol_periods_count': extreme_vol_periods.sum()
    }


def optimize_btc_kalman_parameters(df: pd.DataFrame, 
                                  max_combinations: int = 50) -> Tuple[Dict, float]:
    """ë¹„íŠ¸ì½”ì¸ ì¹¼ë§Œ í•„í„° íŒŒë¼ë¯¸í„° ìµœì í™”"""
    
    logger.info("ğŸ”§ ì¹¼ë§Œ í•„í„° íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
    
    # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ (ì œí•œëœ ì¡°í•©ìœ¼ë¡œ ìµœì í™”)
    param_grid = {
        'base_Q': [0.001, 0.005, 0.01, 0.02],
        'base_R': [0.5, 1.0, 2.0, 5.0],
        'volatility_threshold': [0.05, 0.08, 0.1, 0.15],
        'preservation_factor': [0.6, 0.7, 0.8, 0.9]
    }
    
    best_score = 0
    best_params = None
    combinations_tested = 0
    
    # ì œí•œëœ ì¡°í•©ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    for base_Q in param_grid['base_Q']:
        for base_R in param_grid['base_R']:
            for threshold in param_grid['volatility_threshold']:
                for factor in param_grid['preservation_factor']:
                    
                    if combinations_tested >= max_combinations:
                        break
                    
                    # íŒŒë¼ë¯¸í„°ë¡œ í•„í„°ë§
                    filter_params = {
                        'base_Q': base_Q,
                        'base_R': base_R,
                        'volatility_threshold': threshold,
                        'preservation_factor': factor
                    }
                    
                    try:
                        df_filtered = apply_btc_kalman_filtering(df, params=filter_params)
                        
                        # ì„±ëŠ¥ í‰ê°€
                        performance = validate_kalman_performance(df, df_filtered)
                        
                        # ì¢…í•© ì ìˆ˜ (ë³´ì¡´ìœ¨ 70% + ë…¸ì´ì¦ˆ ì œê±° 30%)
                        score = (
                            performance['preservation_ratio'] * 0.7 + 
                            performance['noise_reduction'] * 0.3
                        )
                        
                        if score > best_score:
                            best_score = score
                            best_params = filter_params
                            
                        combinations_tested += 1
                        
                    except Exception as e:
                        logger.warning(f"íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤íŒ¨: {filter_params}, ì˜¤ë¥˜: {e}")
                        continue
    
    logger.info(f"âœ… íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ (í…ŒìŠ¤íŠ¸ ì¡°í•©: {combinations_tested})")
    logger.info(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
    logger.info(f"ìµœì  ì ìˆ˜: {best_score:.4f}")
    
    return best_params, best_score


def get_dynamic_kalman_params(market_condition: str) -> Dict:
    """ì‹œì¥ ìƒí™©ë³„ ë™ì  íŒŒë¼ë¯¸í„°"""
    
    base_params = {
        'base_Q': 0.005,
        'base_R': 2.0,
        'volatility_threshold': 0.08,
        'preservation_factor': 0.75,
        'volatility_window': 20,
        'adaptive_factor': 5.0
    }
    
    if market_condition == 'bull_market':
        # ìƒìŠ¹ì¥: ë” ì ê·¹ì ì¸ í•„í„°ë§
        base_params['base_Q'] *= 0.5
        base_params['preservation_factor'] *= 0.9
        
    elif market_condition == 'bear_market':
        # í•˜ë½ì¥: ë³´ìˆ˜ì ì¸ í•„í„°ë§
        base_params['base_Q'] *= 2.0
        base_params['preservation_factor'] *= 1.1
        
    elif market_condition == 'sideways':
        # íš¡ë³´ì¥: ê· í˜•ì¡íŒ í•„í„°ë§
        pass  # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ìœ ì§€
        
    return base_params


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª ë¹„íŠ¸ì½”ì¸ ì¹¼ë§Œ í•„í„° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    dates = pd.date_range('2023-01-01', periods=1000, freq='1min')
    np.random.seed(42)
    
    # ë¹„íŠ¸ì½”ì¸ ê°€ê²© ì‹œë®¬ë ˆì´ì…˜ (ë†’ì€ ë³€ë™ì„±)
    base_price = 50000
    returns = np.random.normal(0, 0.02, len(dates))  # 2% ì¼ì¼ ë³€ë™ì„±
    prices = base_price * np.exp(np.cumsum(returns))
    
    # ê·¹í•œ ë³€ë™ì„± êµ¬ê°„ ì¶”ê°€
    extreme_periods = [200, 400, 600, 800]
    for period in extreme_periods:
        prices[period:period+10] *= np.random.uniform(0.9, 1.1, 10)
    
    test_df = pd.DataFrame({
        'open': prices * np.random.uniform(0.999, 1.001, len(prices)),
        'high': prices * np.random.uniform(1.001, 1.005, len(prices)),
        'low': prices * np.random.uniform(0.995, 0.999, len(prices)),
        'close': prices,
        'volume': np.random.uniform(1000, 5000, len(prices))
    }, index=dates)
    
    # ì¹¼ë§Œ í•„í„° ì ìš©
    filtered_df = apply_btc_kalman_filtering(test_df)
    
    # ì„±ëŠ¥ ê²€ì¦
    performance = validate_kalman_performance(test_df, filtered_df)
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ") 