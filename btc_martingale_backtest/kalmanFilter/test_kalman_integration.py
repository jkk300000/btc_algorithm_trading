#!/usr/bin/env python3
"""
ë¹„íŠ¸ì½”ì¸ ì¹¼ë§Œ í•„í„° í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì¹¼ë§Œ í•„í„°ê°€ í†µí•©ëœ feature_engineering ëª¨ë“ˆì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import os
import sys
from datetime import datetime, timedelta
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_test_data(output_path='test_btc_data.csv', periods=1000):
    """í…ŒìŠ¤íŠ¸ìš© ë¹„íŠ¸ì½”ì¸ ë°ì´í„° ìƒì„±"""
    
    logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ë‚ ì§œ ë²”ìœ„ ìƒì„±
    start_date = datetime(2023, 1, 1)
    dates = [start_date + timedelta(minutes=i) for i in range(periods)]
    
    # ë¹„íŠ¸ì½”ì¸ ê°€ê²© ì‹œë®¬ë ˆì´ì…˜ (ë†’ì€ ë³€ë™ì„±)
    np.random.seed(42)
    base_price = 50000
    
    # ê¸°ë³¸ ê°€ê²© ì›€ì§ì„
    returns = np.random.normal(0, 0.02, periods)  # 2% ì¼ì¼ ë³€ë™ì„±
    prices = base_price * np.exp(np.cumsum(returns))
    
    # ê·¹í•œ ë³€ë™ì„± êµ¬ê°„ ì¶”ê°€
    extreme_periods = [200, 400, 600, 800]
    for period in extreme_periods:
        if period + 10 < periods:
            prices[period:period+10] *= np.random.uniform(0.9, 1.1, 10)
    
    # OHLCV ë°ì´í„° ìƒì„±
    test_data = []
    for i, (date, price) in enumerate(zip(dates, prices)):
        # OHLC ìƒì„±
        open_price = price * np.random.uniform(0.999, 1.001)
        high_price = price * np.random.uniform(1.001, 1.005)
        low_price = price * np.random.uniform(0.995, 0.999)
        close_price = price
        
        # ê±°ë˜ëŸ‰ ìƒì„±
        volume = np.random.uniform(1000, 5000)
        
        test_data.append({
            'datetime': date,
            'open': open_price,
            'high': high_price,
            'low': low_price,
            'close': close_price,
            'volume': volume
        })
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(test_data)
    
    # CSV ì €ì¥
    df.to_csv(output_path, index=False)
    logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {output_path}")
    
    return output_path

def test_kalman_filter_integration():
    """ì¹¼ë§Œ í•„í„° í†µí•© í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸš€ ì¹¼ë§Œ í•„í„° í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_file = create_test_data()
    
    # 2. ê¸°ì¡´ ë°©ì‹ (ì¹¼ë§Œ í•„í„° ì—†ìŒ)
    logger.info("ğŸ“Š ê¸°ì¡´ ë°©ì‹ í…ŒìŠ¤íŠ¸ (ì¹¼ë§Œ í•„í„° ì—†ìŒ)")
    from feature_engineering import add_features
    
    df_original = add_features(
        input_path=test_file,
        output_path='test_original_features.csv',
        use_kalman_filter=False,
        diagnose=True
    )
    
    # 3. ì¹¼ë§Œ í•„í„° ì ìš© (ê¸°ë³¸ íŒŒë¼ë¯¸í„°)
    logger.info("ğŸ¯ ì¹¼ë§Œ í•„í„° ì ìš© í…ŒìŠ¤íŠ¸ (ê¸°ë³¸ íŒŒë¼ë¯¸í„°)")
    df_kalman_basic = add_features(
        input_path=test_file,
        output_path='test_kalman_basic_features.csv',
        use_kalman_filter=True,
        kalman_params=None,  # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
        diagnose=True
    )
    
    # 4. ì¹¼ë§Œ í•„í„° ì ìš© (ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)
    logger.info("ğŸ”§ ì¹¼ë§Œ í•„í„° ìµœì í™” í…ŒìŠ¤íŠ¸")
    df_kalman_optimized = add_features(
        input_path=test_file,
        output_path='test_kalman_optimized_features.csv',
        use_kalman_filter=True,
        optimize_kalman=True,  # íŒŒë¼ë¯¸í„° ìµœì í™”
        diagnose=True
    )
    
    # 5. ì„±ëŠ¥ ë¹„êµ
    logger.info("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ë¶„ì„")
    compare_performance(df_original, df_kalman_basic, df_kalman_optimized)
    
    # 6. íŒŒì¼ ì •ë¦¬
    cleanup_test_files()
    
    logger.info("âœ… ì¹¼ë§Œ í•„í„° í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

def compare_performance(df_original, df_kalman_basic, df_kalman_optimized):
    """ì„±ëŠ¥ ë¹„êµ ë¶„ì„"""
    
    logger.info("ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì‹œì‘")
    
    # ë³€ë™ì„± ê³„ì‚°
    def calculate_volatility(df):
        return df['close'].pct_change().rolling(20).std()
    
    vol_original = calculate_volatility(df_original)
    vol_kalman_basic = calculate_volatility(df_kalman_basic)
    vol_kalman_optimized = calculate_volatility(df_kalman_optimized)
    
    # ê·¹í•œ ë³€ë™ì„± êµ¬ê°„ í™•ì¸
    extreme_threshold = 0.1  # 10%
    extreme_original = (vol_original > extreme_threshold).sum()
    extreme_basic = (vol_kalman_basic > extreme_threshold).sum()
    extreme_optimized = (vol_kalman_optimized > extreme_threshold).sum()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š ì¹¼ë§Œ í•„í„° ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print("="*60)
    
    print(f"ğŸ“ˆ í‰ê·  ë³€ë™ì„±:")
    print(f"   ì›ë³¸ ë°ì´í„°: {vol_original.mean():.4f}")
    print(f"   ì¹¼ë§Œ í•„í„° (ê¸°ë³¸): {vol_kalman_basic.mean():.4f}")
    print(f"   ì¹¼ë§Œ í•„í„° (ìµœì í™”): {vol_kalman_optimized.mean():.4f}")
    
    print(f"\nğŸ¯ ê·¹í•œ ë³€ë™ì„± êµ¬ê°„ (>{extreme_threshold*100}%):")
    print(f"   ì›ë³¸ ë°ì´í„°: {extreme_original}ê°œ")
    print(f"   ì¹¼ë§Œ í•„í„° (ê¸°ë³¸): {extreme_basic}ê°œ")
    print(f"   ì¹¼ë§Œ í•„í„° (ìµœì í™”): {extreme_optimized}ê°œ")
    
    print(f"\nğŸ“‰ ë…¸ì´ì¦ˆ ì œê±° íš¨ê³¼:")
    noise_reduction_basic = (vol_original.mean() - vol_kalman_basic.mean()) / vol_original.mean()
    noise_reduction_optimized = (vol_original.mean() - vol_kalman_optimized.mean()) / vol_original.mean()
    print(f"   ì¹¼ë§Œ í•„í„° (ê¸°ë³¸): {noise_reduction_basic:.2%}")
    print(f"   ì¹¼ë§Œ í•„í„° (ìµœì í™”): {noise_reduction_optimized:.2%}")
    
    print(f"\nğŸ”„ ì‹ í˜¸ ë³´ì¡´ìœ¨:")
    preservation_basic = vol_kalman_basic.mean() / vol_original.mean()
    preservation_optimized = vol_kalman_optimized.mean() / vol_original.mean()
    print(f"   ì¹¼ë§Œ í•„í„° (ê¸°ë³¸): {preservation_basic:.2%}")
    print(f"   ì¹¼ë§Œ í•„í„° (ìµœì í™”): {preservation_optimized:.2%}")
    
    print("="*60)

def cleanup_test_files():
    """í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬"""
    
    test_files = [
        'test_btc_data.csv',
        'test_original_features.csv',
        'test_kalman_basic_features.csv',
        'test_kalman_optimized_features.csv'
    ]
    
    for file in test_files:
        if os.path.exists(file):
            os.remove(file)
            logger.info(f"ğŸ—‘ï¸ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‚­ì œ: {file}")

def test_kalman_parameters():
    """ì¹¼ë§Œ í•„í„° íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸ”§ ì¹¼ë§Œ í•„í„° íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸")
    
    from kalman_filter_btc import get_dynamic_kalman_params
    
    # ì‹œì¥ ìƒí™©ë³„ íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸
    market_conditions = ['bull_market', 'bear_market', 'sideways']
    
    for condition in market_conditions:
        params = get_dynamic_kalman_params(condition)
        logger.info(f"ğŸ“Š {condition} íŒŒë¼ë¯¸í„°: {params}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸš€ ë¹„íŠ¸ì½”ì¸ ì¹¼ë§Œ í•„í„° í†µí•© í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    try:
        # 1. ê¸°ë³¸ í†µí•© í…ŒìŠ¤íŠ¸
        test_kalman_filter_integration()
        
        # 2. íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸
        test_kalman_parameters()
        
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 