import pandas as pd
import numpy as np
from ta.momentum import RSIIndicator
from ta.volatility import AverageTrueRange
from ta.trend import EMAIndicator
from squeeze_momentum_core import squeeze_momentum_core
import talib
import matplotlib.pyplot as plt
import os
import sys
import time
import logging
from datetime import datetime

# ë©”ì¸ ëª¨ë“ˆ(ì§ì ‘ ì‹¤í–‰í•˜ëŠ” íŒŒì¼)ì€ ë°˜ë“œì‹œ ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
# ì§ì ‘ ì‹¤í–‰ ì‹œ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€í•˜ì—¬ ì ˆëŒ€ ê²½ë¡œ import ê°€ëŠ¥í•˜ê²Œ í•¨
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from kalmanFilter.kalman_filter_btc import apply_btc_kalman_filtering, validate_kalman_performance, optimize_btc_kalman_parameters

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.hasHandlers():
    handler = logging.StreamHandler()
    formatter = logging.Formatter('[%(asctime)s][%(levelname)s] %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)


def calculate_rma(data, period):
    """
    RMA (Relative Moving Average) ê³„ì‚° - Pine Script ta.rma()ì™€ ì •í™•íˆ ì¼ì¹˜
    Pine Script ë¬¸ì„œì— ë”°ë¥¸ ì •í™•í•œ êµ¬í˜„
    
    Args:
        data: pandas Series - ê³„ì‚°í•  ë°ì´í„°
        period: int - ê¸°ê°„
    
    Returns:
        pandas Series - RMA ê°’
    """
    if len(data) < period:
        return pd.Series([np.nan] * len(data), index=data.index)
    
    # ê²°ê³¼ë¥¼ ì €ì¥í•  Series ìƒì„±
    rma = pd.Series(index=data.index, dtype=float)
    
    # Pine Script ta.rma()ì™€ ë™ì¼í•œ ì´ˆê¸°í™”
    # ì²« ë²ˆì§¸ ê°’ì€ ì²« ë²ˆì§¸ ë°ì´í„°ë¡œ ì´ˆê¸°í™”
    rma.iloc[0] = data.iloc[0]
    
    # RMA ê³„ì‚°: Pine Scriptì™€ ì •í™•íˆ ë™ì¼í•œ ë°©ì‹
    # RMA = (prev_rma * (period - 1) + current_value) / period
    for i in range(1, len(data)):
        if i < period:
            # period ë¯¸ë§Œì¼ ë•ŒëŠ” ë‹¨ìˆœ í‰ê·  (Pine Scriptì™€ ë™ì¼)
            rma.iloc[i] = data.iloc[:i+1].mean()
        else:
            # period ì´ìƒì¼ ë•ŒëŠ” RMA ê³µì‹ ì‚¬ìš©
            # Pine Script: (prev_rma * (period - 1) + current_value) / period
            prev_rma = rma.iloc[i-1]
            current_value = data.iloc[i]
            rma.iloc[i] = (prev_rma * (period - 1) + current_value) / period
    
    return rma


def calculate_atr_pinescript(df, period=14):
    """
    Pine Script ta.atr()ì™€ ì •í™•íˆ ë™ì¼í•œ ATR ê³„ì‚°
    íŠ¸ë ˆì´ë”©ë·° íŒŒì¸ìŠ¤í¬ë¦½íŠ¸ì™€ 100% ì¼ì¹˜í•˜ëŠ” êµ¬í˜„
    
    Args:
        df: pandas DataFrame - OHLC ë°ì´í„°
        period: int - ATR ê¸°ê°„ (ê¸°ë³¸ê°’: 14)
    
    Returns:
        pandas Series - ATR ê°’
    """
    # True Range ê³„ì‚° (Pine Scriptì™€ ì •í™•íˆ ë™ì¼)
    high = df['high']
    low = df['low']
    close = df['close']
    
    # ì´ì „ ì¢…ê°€ ê³„ì‚° (ì²« ë²ˆì§¸ ê°’ì€ NaN)
    prev_close = close.shift(1)
    
    # True Rangeì˜ ì„¸ ê°€ì§€ êµ¬ì„±ìš”ì†Œ
    tr1 = high - low
    tr2 = np.abs(high - prev_close)
    tr3 = np.abs(low - prev_close)
    
    # ìµœëŒ€ê°’ ì„ íƒ
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    
    # RMA ê¸°ë°˜ ATR (Pine Scriptì™€ ì •í™•íˆ ë™ì¼)
    atr = calculate_rma(tr, period)
    return atr


def calculate_atr_for_backtest_period(df, backtest_start_date, period=14):
    """
    ë°±í…ŒìŠ¤íŒ… ê¸°ê°„ì— ë§ì¶° ATR ê³„ì‚°
    ì§€ì •ëœ ì‹œì‘ì¼ë¶€í„°ì˜ ë°ì´í„°ë¡œë§Œ ATR ê³„ì‚°
    
    Args:
        df: pandas DataFrame - ì „ì²´ OHLC ë°ì´í„°
        backtest_start_date: str - ë°±í…ŒìŠ¤íŒ… ì‹œì‘ì¼ (ì˜ˆ: '2022-09-01')
        period: int - ATR ê¸°ê°„ (ê¸°ë³¸ê°’: 14)
    
    Returns:
        pandas Series - ATR ê°’ (ì „ì²´ ë°ì´í„° ê¸¸ì´, ë°±í…ŒìŠ¤íŒ… ì‹œì‘ì¼ ì´ì „ì€ NaN)
    """
    # ë°±í…ŒìŠ¤íŒ… ì‹œì‘ì¼ ì´í›„ ë°ì´í„°ë§Œ ì¶”ì¶œ
    backtest_start = pd.to_datetime(backtest_start_date)
    df_backtest = df[df.index >= backtest_start]
    
    if len(df_backtest) == 0:
        logger.warning(f"âš ï¸ ë°±í…ŒìŠ¤íŒ… ì‹œì‘ì¼ {backtest_start_date} ì´í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.Series([np.nan] * len(df), index=df.index)
    
    # ë°±í…ŒìŠ¤íŒ… ê¸°ê°„ ë°ì´í„°ë¡œ ATR ê³„ì‚°
    atr_backtest = calculate_atr_pinescript(df_backtest, period)
    
    # ì „ì²´ ë°ì´í„° ê¸¸ì´ì— ë§ì¶° ê²°ê³¼ ìƒì„± (ë°±í…ŒìŠ¤íŒ… ì‹œì‘ì¼ ì´ì „ì€ NaN)
    atr_full = pd.Series([np.nan] * len(df), index=df.index)
    atr_full.loc[df_backtest.index] = atr_backtest
    
    logger.info(f"âœ… ë°±í…ŒìŠ¤íŒ… ê¸°ê°„ ATR ê³„ì‚° ì™„ë£Œ: {backtest_start_date}ë¶€í„° {len(df_backtest)}ê°œ ë°ì´í„°")
    
    return atr_full


def add_features(input_path, output_path=None, diagnose=True, use_pinescript_atr=True, 
                use_kalman_filter=False, kalman_params=None, optimize_kalman=False, use_multi_scale=False,
                use_gpu=False, n_jobs=1, backtest_start_date=None):
    """
    ta_lib ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ ì§„ì… ì¡°ê±´ ë° ml ê°€ê²© ìƒìŠ¹ ë° í•˜ë½ ì˜ˆì¸¡ì— ì‚¬ìš©í•  íŠ¹ì • ì§€í‘œë¥¼ ê³„ì‚°.
    
    Args:
        input_path: str - ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ
        output_path: str - ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        diagnose: bool - ì§„ë‹¨ ì •ë³´ ì¶œë ¥ ì—¬ë¶€
        use_pinescript_atr: bool - Pine Scriptì™€ ë™ì¼í•œ ATR ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        use_kalman_filter: bool - ì¹¼ë§Œ í•„í„° ì ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        kalman_params: dict - ì¹¼ë§Œ í•„í„° íŒŒë¼ë¯¸í„° (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        optimize_kalman: bool - ì¹¼ë§Œ í•„í„° íŒŒë¼ë¯¸í„° ìµœì í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        use_multi_scale: bool - ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì¹¼ë§Œ í•„í„° ì ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
    """
    start_time = time.time()
    logger.info("="*60)
    logger.info("ğŸš€ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    logger.info("="*60)
    
    logger.info(f"ğŸ“– ì…ë ¥ íŒŒì¼: {input_path}")
    logger.info(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path if output_path else 'ì €ì¥ ì•ˆí•¨'}")
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = os.path.getsize(input_path) / (1024*1024)  # MB
    logger.info(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
    
    # ì„¤ì • ì •ë³´ ë¡œê¹…
    logger.info("âš™ï¸ ì„¤ì • ì •ë³´:")
    logger.info(f"  - Pine Script ATR: {use_pinescript_atr}")
    logger.info(f"  - ì¹¼ë§Œ í•„í„° ì‚¬ìš©: {use_kalman_filter}")
    logger.info(f"  - ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ í•„í„°: {use_multi_scale}")
    logger.info(f"  - íŒŒë¼ë¯¸í„° ìµœì í™”: {optimize_kalman}")
    if kalman_params:
        logger.info(f"  - ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„°: {kalman_params}")
    
    # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ ë¹„í™œì„±í™” - ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
    logger.info("ğŸ”„ ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    
    result = add_features_single(input_path, output_path, diagnose, use_pinescript_atr, 
                               use_kalman_filter, kalman_params, optimize_kalman, use_multi_scale,
                               use_gpu, n_jobs, backtest_start_date)
    
    # ì´ ì†Œìš” ì‹œê°„ ê³„ì‚°
    total_time = time.time() - start_time
    logger.info("="*60)
    logger.info(f"âœ… íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
    logger.info("="*60)
    
    return result


def add_features_single(input_path, output_path=None, diagnose=True, use_pinescript_atr=True,
                       use_kalman_filter=False, kalman_params=None, optimize_kalman=False, use_multi_scale=False,
                       use_gpu=False, n_jobs=1, backtest_start_date=None):
    """
    ì¼ë°˜ íŒŒì¼ ì²˜ë¦¬ (ë‹¨ì¼ íŒŒì¼) - ê°œì„ ëœ ë²„ì „
    
    Args:
        input_path: str - ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ
        output_path: str - ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ
        diagnose: bool - ì§„ë‹¨ ì •ë³´ ì¶œë ¥ ì—¬ë¶€
        use_pinescript_atr: bool - Pine Script ATR ì‚¬ìš© ì—¬ë¶€
        use_kalman_filter: bool - ì¹¼ë§Œ í•„í„° ì ìš© ì—¬ë¶€
        kalman_params: dict - ì¹¼ë§Œ í•„í„° íŒŒë¼ë¯¸í„°
        optimize_kalman: bool - ì¹¼ë§Œ í•„í„° íŒŒë¼ë¯¸í„° ìµœì í™” ì—¬ë¶€
        use_multi_scale: bool - ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì¹¼ë§Œ í•„í„° ì ìš© ì—¬ë¶€
    """
    # ë°ì´í„° ë¡œë”© ë°©ì‹ ê°œì„ 
    try:
        load_start = time.time()
        logger.info("ğŸ“¥ ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        # ë¨¼ì € ì»¬ëŸ¼ êµ¬ì¡° í™•ì¸
        df_sample = pd.read_csv(input_path, nrows=5)
        logger.info(f"ğŸ“Š ì›ë³¸ ì»¬ëŸ¼: {list(df_sample.columns)}")
        logger.info(f"ğŸ“Š ì»¬ëŸ¼ ìˆ˜: {len(df_sample.columns)}")
        
        # datetime ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬
        if 'datetime' in df_sample.columns:
            logger.info("ğŸ“… datetime ì»¬ëŸ¼ ë°œê²¬ - ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
            # datetime ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            df = pd.read_csv(input_path)
            
            # datetime ë³€í™˜ì„ ë” ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            try:
                df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
                # ë³€í™˜ ì‹¤íŒ¨í•œ í–‰ ì œê±°
                original_len = len(df)
                df = df.dropna(subset=['datetime'])
                logger.info(f"âœ… datetime ë³€í™˜ ì™„ë£Œ: {len(df)} í–‰ (ì œê±°ëœ í–‰: {original_len - len(df)})")
                
                # ì‹œê°„ ë²”ìœ„ ì •ë³´
                if len(df) > 0:
                    logger.info(f"ğŸ“… ë°ì´í„° ì‹œê°„ ë²”ìœ„: {df['datetime'].min()} ~ {df['datetime'].max()}")
            except Exception as e:
                logger.warning(f"âš ï¸ datetime ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
                df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d %H:%M:%S', errors='coerce')
                df = df.dropna(subset=['datetime'])
        else:
            # datetime ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©
            df = pd.read_csv(input_path, index_col=0, parse_dates=True)
            # ì¸ë±ìŠ¤ë¥¼ datetime ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
            df['datetime'] = df.index
        
        # timestamp ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'timestamp' not in df.columns:
            df['timestamp'] = df['datetime'].astype(np.int64) // 10**6
        
        print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df)} í–‰")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ì»¬ëŸ¼: {list(df.columns)}")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {str(e)}")
        # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
        df = pd.read_csv(input_path)
        print(f"âš ï¸ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ë¡œë”© ì™„ë£Œ: {len(df)} í–‰")
    
    # OHLC ì»¬ëŸ¼ í™•ì¸
    required_ohlc = ['open', 'high', 'low', 'close']
    missing_ohlc = [col for col in required_ohlc if col not in df.columns]
    if missing_ohlc:
        print(f"âŒ ëˆ„ë½ëœ OHLC ì»¬ëŸ¼: {missing_ohlc}")
        print(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
        return None
    
    # ì¹¼ë§Œ í•„í„° ì ìš© (ì„ íƒì )
    if use_kalman_filter:
        logger.info("-" * 50)
        logger.info("ğŸ¯ ì¹¼ë§Œ í•„í„° ì ìš© ë‹¨ê³„ ì‹œì‘")
        logger.info("-" * 50)
        
        kalman_start = time.time()
        
        # ì›ë³¸ ë°ì´í„° í†µê³„
        logger.info("ğŸ“ˆ ì›ë³¸ ë°ì´í„° í†µê³„:")
        logger.info(f"  - ì¢…ê°€ í‰ê· : {df['close'].mean():.2f}")
        logger.info(f"  - ì¢…ê°€ ë³€ë™ì„±: {df['close'].pct_change().std():.4f}")
        logger.info(f"  - ë°ì´í„° í¬ì¸íŠ¸: {len(df):,}ê°œ")
        
        # íŒŒë¼ë¯¸í„° ìµœì í™” (ì„ íƒì )
        if optimize_kalman:
            logger.info("ğŸ”§ ì¹¼ë§Œ í•„í„° íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
            optimize_start = time.time()
            optimal_params, best_score = optimize_btc_kalman_parameters(df, max_combinations=30)
            optimize_time = time.time() - optimize_start
            kalman_params = optimal_params
            logger.info(f"âœ… íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ (ì†Œìš”ì‹œê°„: {optimize_time:.2f}ì´ˆ)")
            logger.info(f"ğŸ“Š ìµœì  íŒŒë¼ë¯¸í„°: {optimal_params}")
            logger.info(f"ğŸ“Š ìµœì  ì ìˆ˜: {best_score:.4f}")
        else:
            if kalman_params:
                logger.info(f"ğŸ›ï¸ ì‚¬ìš©ì ì§€ì • íŒŒë¼ë¯¸í„°: {kalman_params}")
            else:
                logger.info("ğŸ›ï¸ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
        
        # ì¹¼ë§Œ í•„í„° ì ìš©
        logger.info("ğŸ”„ ì¹¼ë§Œ í•„í„° ì ìš© ì¤‘...")
        filter_start = time.time()
        df_original = df.copy()
        df = apply_btc_kalman_filtering(df, use_multi_scale=use_multi_scale, params=kalman_params, 
                                       use_gpu=use_gpu, n_jobs=n_jobs)
        filter_time = time.time() - filter_start
        logger.info(f"âœ… ì¹¼ë§Œ í•„í„° ì ìš© ì™„ë£Œ (ì†Œìš”ì‹œê°„: {filter_time:.2f}ì´ˆ)")
        
        # ì„±ëŠ¥ ê²€ì¦
        logger.info("ğŸ“Š ì¹¼ë§Œ í•„í„° ì„±ëŠ¥ ê²€ì¦ ì¤‘...")
        performance = validate_kalman_performance(df_original, df)
        logger.info(f"ğŸ“Š ì„±ëŠ¥ ê²°ê³¼:")
        logger.info(f"  - ì‹ í˜¸ ë³´ì¡´ìœ¨: {performance['preservation_ratio']:.2%}")
        logger.info(f"  - ë…¸ì´ì¦ˆ ì œê±°ìœ¨: {performance['noise_reduction']:.2%}")
        logger.info(f"  - ì›ë³¸ ë³€ë™ì„±: {performance['original_volatility_mean']:.4f}")
        logger.info(f"  - í•„í„°ë§ í›„ ë³€ë™ì„±: {performance['filtered_volatility_mean']:.4f}")
        
        # í•„í„°ë§ëœ ë°ì´í„° í†µê³„
        logger.info("ğŸ“ˆ í•„í„°ë§ëœ ë°ì´í„° í†µê³„:")
        logger.info(f"  - ì¢…ê°€ í‰ê· : {df['close'].mean():.2f}")
        logger.info(f"  - ì¢…ê°€ ë³€ë™ì„±: {df['close'].pct_change().std():.4f}")
        
        kalman_total_time = time.time() - kalman_start
        logger.info(f"â±ï¸ ì¹¼ë§Œ í•„í„° ì´ ì†Œìš”ì‹œê°„: {kalman_total_time:.2f}ì´ˆ")
    
    # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹œì‘
    logger.info("-" * 50)
    logger.info("ğŸ“Š ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ë‹¨ê³„ ì‹œì‘")
    logger.info("-" * 50)
    
    indicators_start = time.time()
    
    # RSI ì§€í‘œ ê³„ì‚°
    logger.info("ğŸ“ˆ RSI ì§€í‘œ ê³„ì‚° ì¤‘...")
    rsi_start = time.time()
    df['rsi_7'] = talib.RSI(df['close'], timeperiod=7)
    df['rsi_14'] = talib.RSI(df['close'], timeperiod=14)
    df['rsi_21'] = talib.RSI(df['close'], timeperiod=21)
    logger.info(f"âœ… RSI ì§€í‘œ ê³„ì‚° ì™„ë£Œ ({time.time() - rsi_start:.2f}ì´ˆ)")
    
    # EMA ì§€í‘œ ê³„ì‚°
    logger.info("ğŸ“ˆ EMA ì§€í‘œ ê³„ì‚° ì¤‘...")
    ema_start = time.time()
    df['ema_9'] = talib.EMA(df['close'], timeperiod=9)
    df['ema_21'] = talib.EMA(df['close'], timeperiod=21)
    logger.info(f"âœ… EMA ì§€í‘œ ê³„ì‚° ì™„ë£Œ ({time.time() - ema_start:.2f}ì´ˆ)")
    
    # ATR ê³„ì‚° ë°©ì‹ ì„ íƒ
    logger.info("ğŸ“ˆ ATR ì§€í‘œ ê³„ì‚° ì¤‘...")
    atr_start = time.time()
    if use_pinescript_atr:
        if backtest_start_date:
            logger.info(f"ğŸ”„ ë°±í…ŒìŠ¤íŒ… ê¸°ê°„ ATR ê³„ì‚°: {backtest_start_date}ë¶€í„°...")
            try:
                df['atr_14'] = calculate_atr_for_backtest_period(df, backtest_start_date, 14)
                logger.info(f"âœ… ë°±í…ŒìŠ¤íŒ… ê¸°ê°„ ATR ê³„ì‚° ì™„ë£Œ ({time.time() - atr_start:.2f}ì´ˆ)")
            except Exception as e:
                logger.warning(f"âš ï¸ ë°±í…ŒìŠ¤íŒ… ê¸°ê°„ ATR ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                logger.info("ğŸ”„ ì „ì²´ ë°ì´í„° ATRë¡œ ëŒ€ì²´...")
                df['atr_14'] = calculate_atr_pinescript(df, 14)
                logger.info(f"âœ… ì „ì²´ ë°ì´í„° ATR ê³„ì‚° ì™„ë£Œ ({time.time() - atr_start:.2f}ì´ˆ)")
        else:
            logger.info("ğŸ”„ Pine Scriptì™€ ë™ì¼í•œ RMA ê¸°ë°˜ ATR ê³„ì‚°...")
            try:
                df['atr_14'] = calculate_atr_pinescript(df, 14)
                logger.info(f"âœ… RMA ê¸°ë°˜ ATR ê³„ì‚° ì™„ë£Œ ({time.time() - atr_start:.2f}ì´ˆ)")
            except Exception as e:
                logger.warning(f"âš ï¸ RMA ATR ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                logger.info("ğŸ”„ talib ATRë¡œ ëŒ€ì²´...")
                df['atr_14'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)
                logger.info(f"âœ… talib ATR ê³„ì‚° ì™„ë£Œ ({time.time() - atr_start:.2f}ì´ˆ)")
    else:
        logger.info("ğŸ”„ talib ê¸°ë°˜ SMA ATR ê³„ì‚°...")
        df['atr_14'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)
        logger.info(f"âœ… talib ATR ê³„ì‚° ì™„ë£Œ ({time.time() - atr_start:.2f}ì´ˆ)")
    
    # Squeeze Momentum ê³„ì‚°
    logger.info("ğŸ“ˆ Squeeze Momentum ê³„ì‚° ì¤‘...")
    val_start = time.time()
    try:
        df['val'] = squeeze_momentum_core(df)
        logger.info(f"âœ… Squeeze Momentum ê³„ì‚° ì™„ë£Œ ({time.time() - val_start:.2f}ì´ˆ)")
    except Exception as e:
        logger.warning(f"âš ï¸ Squeeze Momentum ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        # ê°„ë‹¨í•œ val ê³„ì‚° (ì„ì‹œ)
        df['val'] = df['close'] - df['close'].rolling(20).mean()
        logger.warning(f"âš ï¸ ì„ì‹œ val ê³„ì‚° ì‚¬ìš© ({time.time() - val_start:.2f}ì´ˆ)")
    
    # Bollinger Bands ê³„ì‚°
    logger.info("ğŸ“ˆ Bollinger Bands ê³„ì‚° ì¤‘...")
    bb_start = time.time()
    basis, bb_upper, bb_lower = talib.BBANDS(df['close'], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)
    df['bb_basis'] = basis
    df['bb_upper'] = bb_upper
    df['bb_lower'] = bb_lower
    logger.info(f"âœ… Bollinger Bands ê³„ì‚° ì™„ë£Œ ({time.time() - bb_start:.2f}ì´ˆ)")
    
    # SMA ê³„ì‚°
    logger.info("ğŸ“ˆ SMA ê³„ì‚° ì¤‘...")
    sma_start = time.time()
    df['sma_20'] = df['close'].rolling(window=20, min_periods=1).mean()
    logger.info(f"âœ… SMA ê³„ì‚° ì™„ë£Œ ({time.time() - sma_start:.2f}ì´ˆ)")
    
    # ìƒ‰ìƒ ì‹ í˜¸ ê³„ì‚°
    logger.info("ğŸ“ˆ ìƒ‰ìƒ ì‹ í˜¸ ê³„ì‚° ì¤‘...")
    color_start = time.time()
    df['bcolor'] = (df['val'] > 0).astype(int)
    df['scolor'] = (df['val'] < 0).astype(int)
    logger.info(f"âœ… ìƒ‰ìƒ ì‹ í˜¸ ê³„ì‚° ì™„ë£Œ ({time.time() - color_start:.2f}ì´ˆ)")
    
    # Volume ë°ì´í„° ì²˜ë¦¬
    logger.info("ğŸ“ˆ Volume ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    if 'volume' not in df.columns:
        logger.warning("âš ï¸ Volume ì»¬ëŸ¼ì´ ì—†ì–´ì„œ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤")
        df['volume'] = 0
    else:
        logger.info(f"âœ… Volume ë°ì´í„° í™•ì¸ë¨ (í‰ê· : {df['volume'].mean():,.0f})")
    
    # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ
    indicators_time = time.time() - indicators_start
    logger.info(f"â±ï¸ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì´ ì†Œìš”ì‹œê°„: {indicators_time:.2f}ì´ˆ")
    
    # ìµœì¢… ë°ì´í„° ê²€ì¦
    logger.info("-" * 50)
    logger.info("ğŸ” ìµœì¢… ë°ì´í„° ê²€ì¦")
    logger.info("-" * 50)
    
    # ìƒì„±ëœ íŠ¹ì„± ê°œìˆ˜ í™•ì¸
    feature_cols = [
        'rsi_7', 'rsi_14', 'rsi_21', 'ema_9', 'ema_21', 'atr_14',
        'val', 'bb_basis', 'bb_upper', 'bb_lower', 'sma_20', 
        'bcolor', 'scolor', 'volume'
    ]
    
    missing_features = [col for col in feature_cols if col not in df.columns]
    if missing_features:
        logger.warning(f"âš ï¸ ëˆ„ë½ëœ íŠ¹ì„±: {missing_features}")
    else:
        logger.info("âœ… ëª¨ë“  í•„ìˆ˜ íŠ¹ì„±ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    logger.info(f"ğŸ“Š ìµœì¢… ë°ì´í„° shape: {df.shape}")
    logger.info(f"ğŸ“Š ìƒì„±ëœ íŠ¹ì„± ìˆ˜: {len([col for col in feature_cols if col in df.columns])}")
    
    # NaN ê°’ í™•ì¸
    nan_counts = df.isnull().sum()
    if nan_counts.sum() > 0:
        logger.warning("âš ï¸ NaN ê°’ì´ ë°œê²¬ëœ ì»¬ëŸ¼:")
        for col, count in nan_counts[nan_counts > 0].items():
            logger.warning(f"  - {col}: {count}ê°œ ({count/len(df)*100:.1f}%)")
    else:
        logger.info("âœ… NaN ê°’ì´ ì—†ìŠµë‹ˆë‹¤")
    
    # ì§„ë‹¨ ì •ë³´ ì¶œë ¥
    if diagnose:
        logger.info("ğŸ“‹ ìƒì„¸ ì§„ë‹¨ ì •ë³´ ì¶œë ¥ ì¤‘...")
        print_diagnostic_info(df)
    
    # íŒŒì¼ ì €ì¥
    if output_path:
        logger.info("-" * 50)
        logger.info("ğŸ’¾ íŒŒì¼ ì €ì¥ ë‹¨ê³„")
        logger.info("-" * 50)
        
        save_start = time.time()
        logger.info(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {output_path}")
        logger.info(f"ğŸ“Š ì €ì¥í•  ë°ì´í„°: {df.shape[0]:,}í–‰ x {df.shape[1]}ì»¬ëŸ¼")
        
        df.to_csv(output_path, index=False)
        save_time = time.time() - save_start
        
        # ì €ì¥ëœ íŒŒì¼ í¬ê¸° í™•ì¸
        saved_size = os.path.getsize(output_path) / (1024*1024)  # MB
        logger.info(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ")
        logger.info(f"ğŸ“ ì €ì¥ëœ íŒŒì¼ í¬ê¸°: {saved_size:.2f} MB")
        logger.info(f"â±ï¸ ì €ì¥ ì†Œìš”ì‹œê°„: {save_time:.2f}ì´ˆ")
    
    return df


def add_features_chunked(input_path, output_path=None, diagnose=True, use_pinescript_atr=True):
    """
    ì²­í¬ ë‹¨ìœ„ë¡œ íŒŒì¼ ì²˜ë¦¬ (ëŒ€ìš©ëŸ‰ íŒŒì¼ìš©)
    """
    print("ğŸ”„ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ ì‹œì‘...")
    
    # ì²­í¬ í¬ê¸° ì„¤ì •
    chunk_size = 50000
    total_rows = 0
    first_chunk = True
    
    # ì¶œë ¥ íŒŒì¼ ì´ˆê¸°í™”
    if output_path:
        # í—¤ë”ë§Œ ë¨¼ì € ìƒì„±
        df_sample = pd.read_csv(input_path, nrows=1)
        if 'datetime' in df_sample.columns:
            df_sample = pd.read_csv(input_path, nrows=1)
            df_sample['datetime'] = pd.to_datetime(df_sample['datetime'])
        else:
            df_sample = pd.read_csv(input_path, nrows=1, index_col=0, parse_dates=True)
            df_sample['datetime'] = df_sample.index
        
        if 'timestamp' not in df_sample.columns:
            df_sample['timestamp'] = df_sample['datetime'].astype(np.int64) // 10**6
        
        # ë”ë¯¸ ë°ì´í„°ë¡œ í—¤ë” ìƒì„±
        dummy_df = pd.DataFrame(columns=df_sample.columns)
        dummy_df.to_csv(output_path, index=False)
    
    for chunk in pd.read_csv(input_path, chunksize=chunk_size):
        print(f"ğŸ”„ ì²­í¬ ì²˜ë¦¬ ì¤‘... (í˜„ì¬ í–‰: {total_rows:,})")
        
        # datetime ì»¬ëŸ¼ ì²˜ë¦¬
        if 'datetime' in chunk.columns:
            chunk['datetime'] = pd.to_datetime(chunk['datetime'])
        else:
            # ì¸ë±ìŠ¤ë¥¼ datetimeìœ¼ë¡œ ì‚¬ìš©
            chunk['datetime'] = pd.to_datetime(chunk.index)
        
        # timestamp ì»¬ëŸ¼ ì²˜ë¦¬
        if 'timestamp' not in chunk.columns:
            chunk['timestamp'] = chunk['datetime'].astype(np.int64) // 10**6
        
        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        chunk['rsi_7'] = talib.RSI(chunk['close'], timeperiod=7)
        chunk['rsi_14'] = talib.RSI(chunk['close'], timeperiod=14)
        chunk['rsi_21'] = talib.RSI(chunk['close'], timeperiod=21)
        chunk['ema_9'] = talib.EMA(chunk['close'], timeperiod=9)
        chunk['ema_21'] = talib.EMA(chunk['close'], timeperiod=21)
        
        # ATR ê³„ì‚° - ê° ì²­í¬ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°
        if use_pinescript_atr:
            print(f"   ğŸ”„ ATR ê³„ì‚° ì¤‘... (ì²­í¬ {total_rows//chunk_size + 1})")
            try:
                # ê° ì²­í¬ì—ì„œ ATR ê³„ì‚°
                atr_chunk = calculate_atr_pinescript(chunk, 14)
                chunk['atr_14'] = atr_chunk
                print(f"   âœ… ATR ê³„ì‚° ì™„ë£Œ - í‰ê· : {atr_chunk.mean():.4f}, ê²°ì¸¡ì¹˜: {atr_chunk.isna().sum()}")
            except Exception as e:
                print(f"   âŒ ATR ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
                # talib ATRë¡œ ëŒ€ì²´
                chunk['atr_14'] = talib.ATR(chunk['high'], chunk['low'], chunk['close'], timeperiod=14)
                print(f"   âš ï¸ talib ATR ì‚¬ìš© - í‰ê· : {chunk['atr_14'].mean():.4f}")
        else:
            chunk['atr_14'] = talib.ATR(chunk['high'], chunk['low'], chunk['close'], timeperiod=14)
        
        # val ê³„ì‚°
        try:
            chunk['val'] = squeeze_momentum_core(chunk)
        except Exception as e:
            chunk['val'] = chunk['close'] - chunk['close'].rolling(20).mean()
        
        # Bollinger Bands
        basis, bb_upper, bb_lower = talib.BBANDS(chunk['close'], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)
        chunk['bb_basis'] = basis
        chunk['bb_upper'] = bb_upper
        chunk['bb_lower'] = bb_lower
        
        # SMA(20)
        chunk['sma_20'] = chunk['close'].rolling(window=20, min_periods=1).mean()
        
        # bcolor, scolor
        chunk['bcolor'] = (chunk['val'] > 0).astype(int)
        chunk['scolor'] = (chunk['val'] < 0).astype(int)
        
        # volume
        if 'volume' not in chunk.columns:
            chunk['volume'] = 0
        
        # CSV íŒŒì¼ì— ì €ì¥
        if output_path:
            if first_chunk:
                chunk.to_csv(output_path, index=False, mode='w')
                first_chunk = False
            else:
                chunk.to_csv(output_path, index=False, mode='a', header=False)
        
        total_rows += len(chunk)
        
        # ì§„í–‰ìƒí™© ì¶œë ¥
        if total_rows % 100000 == 0:
            print(f"   ì²˜ë¦¬ëœ í–‰ ìˆ˜: {total_rows:,}")
    
    print(f"âœ… ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ! ì´ {total_rows:,} í–‰")
    
    if diagnose and output_path:
        # ì§„ë‹¨ì„ ìœ„í•´ ìƒ˜í”Œ ì½ê¸°
        df_sample = pd.read_csv(output_path, nrows=10000)
        print_diagnostic_info(df_sample)
    
    return None  # ì²­í¬ ì²˜ë¦¬ì—ì„œëŠ” DataFrameì„ ë°˜í™˜í•˜ì§€ ì•ŠìŒ


def print_diagnostic_info(df):
    """
    ì§„ë‹¨ ì •ë³´ ì¶œë ¥
    """
    print("\n[add_features] ë°ì´í„° ì§„ë‹¨ ê²°ê³¼:")
    print(f"ì „ì²´ í–‰ ê°œìˆ˜: {len(df)}")
    print("í”¼ì²˜ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜:")
    print(df.isna().sum())
    print("í”¼ì²˜ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨(%):")
    print((df.isna().sum() / len(df) * 100).round(2))
    print("ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” í–‰ ê°œìˆ˜:", len(df.dropna()))
    
    # ATR í†µê³„ ì •ë³´
    print("\nğŸ“Š ATR í†µê³„ ì •ë³´:")
    print(f"ATR í‰ê· : {df['atr_14'].mean():.4f}")
    print(f"ATR í‘œì¤€í¸ì°¨: {df['atr_14'].std():.4f}")
    print(f"ATR ìµœì†Œê°’: {df['atr_14'].min():.4f}")
    print(f"ATR ìµœëŒ€ê°’: {df['atr_14'].max():.4f}")
    print(f"ATR ê²°ì¸¡ì¹˜: {df['atr_14'].isna().sum()}")
    
    # val í†µê³„ ì •ë³´
    print("\nğŸ“Š val í†µê³„ ì •ë³´:")
    print(f"val í‰ê· : {df['val'].mean():.4f}")
    print(f"val í‘œì¤€í¸ì°¨: {df['val'].std():.4f}")
    print(f"val ìµœì†Œê°’: {df['val'].min():.4f}")
    print(f"val ìµœëŒ€ê°’: {df['val'].max():.4f}")
    print(f"val ê²°ì¸¡ì¹˜: {df['val'].isna().sum()}")
    
    # datetime ë²”ìœ„
    print(f"\nâ° ë°ì´í„° ë²”ìœ„:")
    print(f"ì‹œì‘: {df['datetime'].min()}")
    print(f"ì¢…ë£Œ: {df['datetime'].max()}")


def check_atr_at_specific_time(df, target_datetime=None, target_index=None):
    """
    íŠ¹ì • ì‹œì ì˜ ATR ê°’ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        df: pandas DataFrame - OHLC ë°ì´í„°
        target_datetime: str - ëª©í‘œ ë‚ ì§œì‹œê°„ (ì˜ˆ: '2025-07-26 08:00:00')
        target_index: int - ëª©í‘œ ì¸ë±ìŠ¤ (datetime ëŒ€ì‹  ì‚¬ìš©)
    """
    print(f"ğŸ” íŠ¹ì • ì‹œì  ATR ê°’ í™•ì¸...")
    
    if target_datetime:
        # datetimeìœ¼ë¡œ ì°¾ê¸°
        target_dt = pd.to_datetime(target_datetime)
        mask = df['datetime'] >= target_dt
        if mask.any():
            target_idx = mask.idxmax()
            print(f"ëª©í‘œ ì‹œê°„: {target_datetime}")
        else:
            print(f"âŒ í•´ë‹¹ ì‹œê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_datetime}")
            return
    elif target_index is not None:
        target_idx = target_index
        print(f"ëª©í‘œ ì¸ë±ìŠ¤: {target_index}")
    else:
        # ë§ˆì§€ë§‰ ê°’ í™•ì¸
        target_idx = df.index[-1]
        print(f"ë§ˆì§€ë§‰ ê°’ í™•ì¸")
    
    # í•´ë‹¹ ì‹œì ì˜ ë°ì´í„° í™•ì¸
    target_row = df.loc[target_idx]
    print(f"ğŸ“Š í•´ë‹¹ ì‹œì  ë°ì´í„°:")
    print(f"  ì‹œê°„: {target_row['datetime']}")
    print(f"  OHLC: {target_row['open']:.2f}, {target_row['high']:.2f}, {target_row['low']:.2f}, {target_row['close']:.2f}")
    
    # ATR ê³„ì‚° (í•´ë‹¹ ì‹œì ê¹Œì§€ì˜ ë°ì´í„°ë¡œ)
    df_subset = df.loc[:target_idx]
    atr_values = calculate_atr_pinescript(df_subset, 14)
    current_atr = atr_values.iloc[-1]
    
    print(f"ğŸ“Š ATR ê°’:")
    print(f"  í˜„ì¬ ê³„ì‚°ëœ ATR: {current_atr:.6f}")
    
    # True Range ê°’ë“¤ í™•ì¸ (ìµœê·¼ 5ê°œ)
    high = df_subset['high'].values
    low = df_subset['low'].values
    close = df_subset['close'].values
    
    print(f"\nğŸ“Š ìµœê·¼ 5ê°œ True Range ê°’:")
    for i in range(max(0, len(df_subset)-5), len(df_subset)):
        if i == 0:
            tr = high[i] - low[i]
        else:
            tr = max(
                high[i] - low[i],
                abs(high[i] - close[i-1]),
                abs(low[i] - close[i-1])
            )
        print(f"  í–‰ {i}: TR = {tr:.6f}")
    
    return current_atr


def verify_atr_calculation(df, target_atr=104.7, tolerance=0.1):
    """
    ATR ê³„ì‚° ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        df: pandas DataFrame - OHLC ë°ì´í„°
        target_atr: float - ëª©í‘œ ATR ê°’ (Pine Script ê°’)
        tolerance: float - í—ˆìš© ì˜¤ì°¨
    
    Returns:
        dict - ê²€ì¦ ê²°ê³¼
    """
    print(f"ğŸ” ATR ê³„ì‚° ê²€ì¦ ì¤‘...")
    print(f"ëª©í‘œ ATR ê°’: {target_atr}")
    
    # í˜„ì¬ ATR ê³„ì‚°
    current_atr = calculate_atr_pinescript(df, 14)
    current_value = current_atr.iloc[-1]  # ë§ˆì§€ë§‰ ê°’
    
    print(f"í˜„ì¬ ê³„ì‚°ëœ ATR: {current_value:.6f}")
    print(f"ì°¨ì´: {abs(current_value - target_atr):.6f}")
    
    # ê²€ì¦ ê²°ê³¼
    is_valid = abs(current_value - target_atr) <= tolerance
    
    result = {
        'current_atr': current_value,
        'target_atr': target_atr,
        'difference': abs(current_value - target_atr),
        'is_valid': is_valid,
        'tolerance': tolerance
    }
    
    if is_valid:
        print(f"âœ… ATR ê°’ì´ ëª©í‘œê°’ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤ (ì˜¤ì°¨: {result['difference']:.6f})")
    else:
        print(f"âŒ ATR ê°’ì´ ëª©í‘œê°’ê³¼ ë‹¤ë¦…ë‹ˆë‹¤ (ì˜¤ì°¨: {result['difference']:.6f})")
        print(f"ğŸ’¡ ê°€ëŠ¥í•œ ì›ì¸:")
        print(f"  1. ë°ì´í„° ì‹œì ì´ ë‹¤ë¦„ (í˜„ì¬: {df['datetime'].iloc[-1]})")
        print(f"  2. ë°ì´í„° ì •ë°€ë„ ì°¨ì´")
        print(f"  3. Pine Scriptì™€ ê³„ì‚° ë°©ì‹ì˜ ë¯¸ì„¸í•œ ì°¨ì´")
    
    return result


def compare_atr_methods(df):
    """
    talib ATRê³¼ Pine Script ATR ë¹„êµ
    
    Args:
        df: pandas DataFrame - OHLC ë°ì´í„°
    """
    print("ğŸ”„ ATR ê³„ì‚° ë°©ì‹ ë¹„êµ ì¤‘...")
    
    # talib ATR (SMA ê¸°ë°˜)
    atr_talib = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)
    
    # Pine Script ATR (RMA ê¸°ë°˜)
    atr_pinescript = calculate_atr_pinescript(df, 14)
    
    # ë¹„êµ ê²°ê³¼
    print("\nğŸ“Š ATR ë¹„êµ ê²°ê³¼:")
    print(f"talib ATR í‰ê· : {atr_talib.mean():.4f}")
    print(f"Pine Script ATR í‰ê· : {atr_pinescript.mean():.4f}")
    print(f"ì°¨ì´ (Pine Script - talib): {(atr_pinescript.mean() - atr_talib.mean()):.4f}")
    print(f"ìƒê´€ê³„ìˆ˜: {atr_pinescript.corr(atr_talib):.4f}")
    
    # ë§ˆì§€ë§‰ ê°’ ë¹„êµ
    print(f"\nğŸ“Š ë§ˆì§€ë§‰ ATR ê°’ ë¹„êµ:")
    print(f"talib ATR ë§ˆì§€ë§‰ ê°’: {atr_talib.iloc[-1]:.6f}")
    print(f"Pine Script ATR ë§ˆì§€ë§‰ ê°’: {atr_pinescript.iloc[-1]:.6f}")
    print(f"ì°¨ì´: {abs(atr_pinescript.iloc[-1] - atr_talib.iloc[-1]):.6f}")
    
    return atr_talib, atr_pinescript


if __name__ == '__main__':
    try:
        print("ğŸš€ Feature Engineering ì‹œì‘...")
        print("=" * 50)
        
        # Pine Scriptì™€ ë™ì¼í•œ ATR ì‚¬ìš© + ì„±ëŠ¥ ìµœì í™”
        # df = add_features('C:/ì„ ë¬¼ë°ì´í„°/binance_btcusdt_1m.csv', 
        #                  'C:/ì„ ë¬¼ë°ì´í„°/binance_btcusdt_1m_features.csv',
        #                  use_pinescript_atr=True, 
        #                  optimize_kalman=False,  # ìµœì í™” ë¹„í™œì„±í™” (ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€) 
        #                  use_kalman_filter=False, 
        #                  use_multi_scale=False,   # 2ë‹¨ê³„ í•„í„° í™œì„±í™” (ë©€í‹° ìŠ¤ì¼€ì¼ ì ìš©)
        #                  use_gpu=True,        # GPU ê°€ì† (CuPy ì„¤ì¹˜ ì‹œ Trueë¡œ ë³€ê²½)
        #                  n_jobs=7,             # ë³‘ë ¬ ì²˜ë¦¬ (CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •)
        #                  kalman_params={
        #                     'base_Q': 0.01,               # ê· í˜•ì¡íŒ ë°˜
        #                     'base_R': 1.0,                # ë¹ ë¥¸ ì‹ í˜¸ ë°˜ì˜
        #                     'volatility_threshold': 0.05,  # ë¯¼ê°í•œ ë³€ë™ì„± ê°ì§€
        #                     'preservation_factor': 0.82,   # ë†’ì€ ì‹ í˜¸ ë³´ì¡´
        #                     'volatility_window': 12,       # ë¹ ë¥¸ ì ì‘09-   (12ë¶„)
        #                     'adaptive_factor': 5.0
        #                     })

        df = add_features('C:/ì„ ë¬¼ë°ì´í„°/binance_btcusdt_1m.csv', 
                         'C:/ì„ ë¬¼ë°ì´í„°/binance_btcusdt_1m_features.csv',
                         use_pinescript_atr=True, use_gpu=True, n_jobs=7)
                         
                                                                            
        # ATR ë°©ì‹ ë¹„êµ (dfê°€ Noneì´ ì•„ë‹ ë•Œë§Œ)
        if df is not None:
            print("\nğŸ”„ ATR ë°©ì‹ ë¹„êµ ì¤‘...")
            compare_atr_methods(df)
            
            # íŠ¹ì • ì‹œì ì˜ ATR ê°’ í™•ì¸ (2025-07-21 00:39:00)
            print("\nğŸ” íŠ¹ì • ì‹œì  ATR ê°’ í™•ì¸...")
            check_atr_at_specific_time(df, target_datetime='2025-07-21 00:39:00')
            
            # ATR ê°’ ê²€ì¦ (Pine Script ê°’ê³¼ ë¹„êµ)
            print("\nğŸ” ATR ê°’ ê²€ì¦ ì¤‘...")
            verify_atr_calculation(df, target_atr=104.7, tolerance=0.1)

            # val ì»¬ëŸ¼ì— ì–‘ìˆ˜ ê°’ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ í™•ì¸
            has_positive_val = (df['val'] > 0).any()

            if has_positive_val:
                print("âœ… val ì»¬ëŸ¼ì— ì–‘ìˆ˜ ê°’ì´ ìˆìŠµë‹ˆë‹¤.")
            else:
                print("âš ï¸ val ì»¬ëŸ¼ì— ì–‘ìˆ˜ ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
                
            print(f"\nâœ… Feature Engineering ì™„ë£Œ!")
            print(f"ğŸ“Š ìµœì¢… ë°ì´í„° í¬ê¸°: {len(df)} í–‰, {len(df.columns)} ì»¬ëŸ¼")
        else:
            print("âš ï¸ DataFrameì´ Noneì…ë‹ˆë‹¤. ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("ìƒì„±ëœ features íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nğŸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")

   